{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2d02ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers pillow torchvision torch safetensors pillow\n",
    "!CFLAGS=\"-mavx -DWARN(a)=(a)\" pip install nmslib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7be1e7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!python --version\n",
    "!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48072c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e29f161",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers --no-cache-dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5267b75b",
   "metadata": {},
   "source": [
    "# Load the CLIP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5ae5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "\n",
    "\n",
    "#repo_id = \"laion/CLIP-ViT-H-14-laion2B-s32B-b79K\"\n",
    "#repo_id = \"laion/CLIP-ViT-g-14-laion2B-s12B-b42K\" # more modern CLIP model\n",
    "repo_id = \"openai/clip-vit-large-patch14-336\" # the CLIP model used for SD up to v1.5\n",
    "device = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "\n",
    "print(\"loading model...\")\n",
    "model = CLIPModel.from_pretrained(repo_id)\n",
    "print(\"loading preprocessor...\")\n",
    "processor = CLIPProcessor.from_pretrained(repo_id)\n",
    "\n",
    "print(f\"sending to {device}...\")\n",
    "if device == 'cuda':\n",
    "    model = model.half()\n",
    "model.to(device)\n",
    "print(\"done\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad6e8e5",
   "metadata": {},
   "source": [
    "## helper code to load images and create CLIP feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488d351c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import PIL\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "from typing import Union\n",
    "\n",
    "\n",
    "def get_clip_image_features(image: PIL.Image) -> torch.Tensor:\n",
    "    preprocess_results = processor(text=None, \n",
    "                                   images=image, \n",
    "                                   return_tensors=\"pt\", \n",
    "                                   padding=True, \n",
    "                                   device=device\n",
    "                                  )\n",
    "    pixel_values = preprocess_results.pixel_values\n",
    "    #print(pixel_values.device)\n",
    "    image_features = model.get_image_features(pixel_values = pixel_values.to(model.device))\n",
    "    return image_features\n",
    "\n",
    "\n",
    "def resize_image(fullsize_image: PIL.Image, min_edge_length: int, max_edge_length: int=None) -> PIL.Image:\n",
    "    transform = transforms.Resize(size=min_edge_length, max_size=max_edge_length)\n",
    "    return transform(fullsize_image)\n",
    "\n",
    "\n",
    "def get_images_feature_vecs(images) -> torch.Tensor:\n",
    "    if type(images) is str or type(images) is bytes:\n",
    "        images = [images]\n",
    "        \n",
    "    #print([type(i) for i in images])\n",
    "    image_sources = [io.BytesIO(i) if type(i) is bytes else i for i in images]\n",
    "    images = [resize_image(PIL.Image.open(i), min_edge_length=512, max_edge_length=768) for i in image_sources]\n",
    "    clip_features = get_clip_image_features(images)\n",
    "\n",
    "    return clip_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4d2834",
   "metadata": {},
   "source": [
    "# `gather_clip_features_recursive`: crawl a folder and collect CLIP vectors\n",
    "\n",
    "Starting at `root_path`, `gather_clip_features_recursive` calculates a CLIP feature vector for every \n",
    "encountered image. When all directories have been processed, a tuple of `hashes, per_image_features` \n",
    "is returned.\n",
    "*  `hashes` contains sha256 hashes for each image, indexed by image path relative to `root_path`.\n",
    "*  `per_image_features` contains a CLIP feature vector for each image, also indexed by image path \n",
    "    relative to `root_path`.\n",
    "\n",
    "If `existing_hashes` is passed, CLIP feature analysis will be skipped if the image hash is already \n",
    "present in `existing_hashes`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b83354d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import traceback\n",
    "from tqdm.notebook import tqdm\n",
    "import hashlib\n",
    "from safetensors.torch import safe_open\n",
    "\n",
    "def gather_clip_features_recursive(root_path: str, existing_hashes: dict=None, check_hashes: bool=True) -> tuple[dict,dict]:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    image_extensions = [\".png\", \".jpg\", \".jpeg\"]\n",
    "    per_image_features = {}\n",
    "    hashes = {}\n",
    "    try:\n",
    "        for directory, _, filenames in os.walk(root_path):\n",
    "            #print('directory:', directory)\n",
    "            image_filenames = [f for f in filenames if os.path.splitext(f)[1] in image_extensions]\n",
    "            if len(image_filenames)==0:\n",
    "                continue\n",
    "            for filename in tqdm(image_filenames, desc=directory):\n",
    "                image_path = os.path.join(directory, filename)\n",
    "                relative_path = \"./\" + os.path.relpath(image_path, root_path).replace('\\\\', '/')\n",
    "                #print(f\"from '{root_path}', '{directory}', '{filename}' got {relative_path}\")\n",
    "                #raise KeyboardInterrupt\n",
    "                if not check_hashes and relative_path in existing_hashes:\n",
    "                    #print(f\"already have entry for {filename}\")\n",
    "                    continue\n",
    "                try:\n",
    "                    image_file_bytes = None\n",
    "                    \n",
    "                    with open(image_path, 'rb') as image_file_handle:\n",
    "                        image_file_bytes = image_file_handle.read() # read entire file as bytes\n",
    "                    shasum = hashlib.sha256(image_file_bytes).hexdigest()\n",
    "\n",
    "                    #print(f\"checking for hash {shasum} in {existing_hashes}...\")\n",
    "                    if existing_hashes is not None and existing_hashes.get(filename, '') == shasum:\n",
    "                        continue\n",
    "                        \n",
    "                    features = get_images_feature_vecs(image_file_bytes).squeeze(0).detach()\n",
    "                    per_image_features[relative_path] = features.detach().cpu().half().numpy()\n",
    "                    hashes[relative_path] = shasum\n",
    "                except (IOError, OSError) as e:\n",
    "                    print(f\"Error loading {image_path}: {e}\")\n",
    "                    continue\n",
    "    except KeyboardInterrupt:\n",
    "        print(f\"interrupted, returning what we have collected so far (= vectors for {len(hashes)} images)\")\n",
    "    except Exception as e:\n",
    "        traceback.print_exc()\n",
    "        print(f\"an exception occurred, saving what we have collected so far (= vectors for {len(hashes)} images)\")\n",
    "\n",
    "    return hashes, per_image_features\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e516d15",
   "metadata": {},
   "source": [
    "### File management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8afa33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def get_clip_features_pickle_path(root_path):\n",
    "    return os.path.join(root_path, \"__clip-features.pickle\")\n",
    "\n",
    "def load_clip_features(root_path):\n",
    "    data_path = get_clip_features_pickle_path(root_path)\n",
    "    hashes = {}\n",
    "    per_image_features = {}\n",
    "    if os.path.exists(data_path):\n",
    "        with open(data_path, 'rb') as f:\n",
    "            existing_dict = pickle.load(f)\n",
    "            #print(f\"loaded from {data_path}: {existing_dict}\")\n",
    "            existing_repo_id = existing_dict.get(\"repo_id\")\n",
    "            if repo_id != existing_repo_id:\n",
    "                raise ImportError(f\"repo id mismatch. saved is {existing_repo_id}, running is {repo_id}\")\n",
    "            \n",
    "            hashes = existing_dict[\"hashes\"]\n",
    "            per_image_features = existing_dict[\"clip_features\"]\n",
    "    else:\n",
    "        print(f\"no pickle file at {data_path}\")\n",
    "    return hashes, per_image_features\n",
    "\n",
    "def save_clip_features(root_path, hashes, per_image_features, repo_id):\n",
    "    output_dict = {\n",
    "        \"repo_id\": repo_id,\n",
    "        \"hashes\": hashes,\n",
    "        \"clip_features\": per_image_features\n",
    "    }\n",
    "    data_path = get_clip_features_pickle_path(root_path)\n",
    "    print(\"saving to \", root_path)\n",
    "    with open(data_path, 'wb') as f:\n",
    "        pickle.dump(output_dict, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        print(\"saved\")\n",
    "\n",
    "def update_clip_features_recursive(root_path, check_hashes=True):\n",
    "    \"\"\"\n",
    "    Crawl root_path and all subdirectories, creating or updating CLIP feature vectors as required.\n",
    "    \"\"\"\n",
    "    print(\"updating features for images in\", root_path)\n",
    "    try:\n",
    "        hashes, per_image_features = load_clip_features(root_path)\n",
    "    except EOFError as e:\n",
    "        print(\"error loading clip feature for {root_path}: {e}\")\n",
    "        hashes = {}\n",
    "        per_image_features = {}\n",
    "    new_hashes, new_per_image_features = gather_clip_features_recursive(root_path, hashes, check_hashes)\n",
    "    if len(new_hashes)>0:\n",
    "        hashes.update(new_hashes)\n",
    "        per_image_features.update(new_per_image_features)\n",
    "        save_clip_features(root_path, hashes, per_image_features, repo_id)\n",
    "        print(f\"wrote vectors for {len(hashes)} images ({len(new_hashes)} new) to {root_path}\")\n",
    "    else:\n",
    "        print(f\"already up to date with {len(hashes)} images\")\n",
    "    return hashes, per_image_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4d269e",
   "metadata": {},
   "source": [
    "# Create/update CLIP features for images\n",
    "\n",
    "This is slow and expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc77afb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# this is slow and expensive\n",
    "root_path = './images'\n",
    "hashes, per_image_features = update_clip_features_recursive(root_path, check_hashes=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b548d9",
   "metadata": {},
   "source": [
    "# Load existing CLIP features for images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016008af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "root_path = './images'\n",
    "hashes, per_image_features = load_clip_features(root_path)\n",
    "print(f\"{len(hashes)} features loaded\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70bd111f",
   "metadata": {},
   "source": [
    "# Make a UI for searching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95172bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nmslib\n",
    "import torch\n",
    "\n",
    "def build_index(all_features_tensor: torch.Tensor) -> nmslib:\n",
    "    print(\"building index for all_features_tensor of shape\", all_features_tensor.shape)\n",
    "    index = nmslib.init(method='hnsw', space='cosinesimil')\n",
    "    index.addDataPointBatch(all_features_tensor.cpu())\n",
    "    index.createIndex({'post': 2}, print_progress=True)\n",
    "    return index\n",
    "\n",
    "print(f\"merging images features into a single tensor...\")\n",
    "all_features_tensor = torch.cat([torch.tensor(f, device=device).unsqueeze(0) for _,f in tqdm(per_image_features.items())])\n",
    "print(\"building knn index...\")\n",
    "index = build_index(all_features_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4878e8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_filename(path):\n",
    "    return os.path.split(path)[-1]\n",
    "\n",
    "def get_closest_images(features: np.array, k=4):\n",
    "    ids, distances = index.knnQuery(features, k=k)\n",
    "    print(\"knnquery got ids\", ids)\n",
    "    return ids, distances\n",
    "\n",
    "def log_closest(ids, distances, remove_directory=False):\n",
    "    nearest_paths = [all_features[i][0] for i in ids]\n",
    "    if remove_directory:\n",
    "        nearest_paths = [get_filename(p) for p in nearest_paths]\n",
    "    print(f\"  {distances[1:]}\")\n",
    "    print(f\"  {nearest_paths[1:]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d4482c",
   "metadata": {},
   "source": [
    "## image 2 image search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c93e8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for (path,features) in per_image_features.items():\n",
    "    print(f\"{get_filename(path)}:\")\n",
    "    ids, distances = get_closest_images(features.detach().cpu())\n",
    "    log_closest(ids, distances)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21096605",
   "metadata": {},
   "source": [
    "## text 2 image search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb1918e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_features(text: str) -> torch.Tensor:\n",
    "\n",
    "    input = processor(text=[text], images=None, return_tensors=\"pt\", padding=True)\n",
    "    #print(input)\n",
    "    input_ids = input.input_ids.to(device)\n",
    "    return model.get_text_features(input_ids=input_ids)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463c951c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "class Timer:\n",
    "    def __init__(self, timeout, callback):\n",
    "        self._timeout = timeout\n",
    "        self._callback = callback\n",
    "\n",
    "    async def _job(self):\n",
    "        await asyncio.sleep(self._timeout)\n",
    "        self._callback()\n",
    "\n",
    "    def start(self):\n",
    "        self._task = asyncio.ensure_future(self._job())\n",
    "\n",
    "    def cancel(self):\n",
    "        self._task.cancel()\n",
    "\n",
    "def debounce(wait):\n",
    "    \"\"\" Decorator that will postpone a function's\n",
    "        execution until after `wait` seconds\n",
    "        have elapsed since the last time it was invoked. \"\"\"\n",
    "    def decorator(fn):\n",
    "        timer = None\n",
    "        def debounced(*args, **kwargs):\n",
    "            nonlocal timer\n",
    "            def call_it():\n",
    "                fn(*args, **kwargs)\n",
    "            if timer is not None:\n",
    "                timer.cancel()\n",
    "            timer = Timer(wait, call_it)\n",
    "            timer.start()\n",
    "        return debounced\n",
    "    return decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf87414",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interact_manual, fixed\n",
    "import PIL\n",
    "\n",
    "import io\n",
    "import subprocess\n",
    "import traceback\n",
    "\n",
    "def open_in_finder(path):\n",
    "    print(\"opening\", path)\n",
    "    subprocess.call([\"open\", \"-R\", path])\n",
    "\n",
    "# Yield successive n-sized\n",
    "# chunks from l.\n",
    "def divide_chunks(l, n):\n",
    "    # looping till length l\n",
    "    for i in range(0, len(l), n):\n",
    "        yield l[i:i + n]\n",
    "\n",
    "def load_and_resize_image(path, target_width) -> PIL.Image:\n",
    "    image = PIL.Image.open(path)\n",
    "\n",
    "    size = image.size\n",
    "    aspect_ratio = size[0]/size[1]\n",
    "\n",
    "    height=round(width/aspect_ratio)\n",
    "    new_size=(width,height)\n",
    "    return image.resize(new_size)\n",
    "\n",
    "        \n",
    "@debounce(2.0)\n",
    "def display_closest(ids, distances, load_more_cb, go_back_cb, in_widget, offset=0):\n",
    "    in_widget.children = []\n",
    "    print(\"displaying\", ids)\n",
    "    all_image_paths = list(per_image_features.keys())\n",
    "    nearest_paths = [os.path.join(root_path, all_image_paths[i]) for i in ids]\n",
    "    print(\"-> displaying\", nearest_paths)\n",
    "    \n",
    "    width = 300\n",
    "    images: list[bytes] = []\n",
    "    aspect_ratios: list[float] = []\n",
    "    for path in nearest_paths:\n",
    "        resized = None\n",
    "        try:\n",
    "            resized = load_and_resize_image(path, width)\n",
    "        except Exception:\n",
    "            traceback.print_exc()\n",
    "            resized = PIL.Image.new('RGB', (100, 100))\n",
    "            \n",
    "        aspect_ratios.append(resized.width/resized.height)\n",
    "        byte_arr = io.BytesIO()\n",
    "        resized.save(byte_arr, format='jpeg')\n",
    "        images.append(byte_arr.getvalue())\n",
    "    \n",
    "    image_widgets = [widgets.Image(value=image, format='png', width=width, height=width/aspect_ratio) \n",
    "                     for (image,aspect_ratio) in zip(image_datas,aspect_ratios)]\n",
    "    #print(\"made image widgets\") \n",
    "    for image_widget in image_widgets:\n",
    "        image_widget.layout.object_fit='contain'\n",
    "    #print(\"made image widgets\") \n",
    "    button_widgets = [widgets.Button(description='open') for _ in nearest_paths]\n",
    "    for i, p in enumerate(nearest_paths):\n",
    "        button_widgets[i].on_click(lambda b, p=p: open_in_finder(p))\n",
    "    #    for i, button_widget in enumerate(button_widgets):\n",
    "    #        button_widget.on_click(lambda path=nearest_paths[i]: print(path))\n",
    "    #print(\"made button widgets\") \n",
    "    widget_lists = [(image_widgets[i],\n",
    "                    widgets.Label(value=p),\n",
    "                    button_widgets[i]) \n",
    "                    for i,p in enumerate(nearest_paths)]\n",
    "    #print(\"made widget lists\") \n",
    "    \n",
    "    vboxes = [widgets.VBox(w) for w in widget_lists]\n",
    "    \n",
    "    row_length = 5\n",
    "    chunks = list(divide_chunks(vboxes, row_length))\n",
    "\n",
    "    hboxes = [widgets.HBox(c) for c in chunks]\n",
    "    \n",
    "    #print(f\"made {len(hboxes)} hboxes\") \n",
    "    #print(\"vboxes:\", vboxes)\n",
    "    \n",
    "    go_back_button = widgets.Button(description=f\"prev ({offset+1-len(nearest_paths)})\")\n",
    "    go_back_button.on_click(go_back_cb)\n",
    "\n",
    "    load_more_button = widgets.Button(description=f\"next ({offset+1+len(nearest_paths)})\")\n",
    "    load_more_button.on_click(load_more_cb)\n",
    "\n",
    "    in_widget.children = [widgets.VBox([go_back_button, load_more_button] + \n",
    "                                       hboxes + \n",
    "                                       [go_back_button, load_more_button])]\n",
    "    #display(in_widget)\n",
    "    \n",
    "\n",
    "def find_nearest_images_for_text(text: str, k, offset, in_widget):\n",
    "    features = get_text_features(text)\n",
    "    query_vec = np.asarray(features.detach().cpu())\n",
    "    #print(features)\n",
    "    ids, distances = get_closest_images(query_vec, k=offset+k)\n",
    "    #print('loading..', offset, len(ids))\n",
    "    next_offset = offset+k\n",
    "    prev_offset = offset-k\n",
    "    def load_more(b):\n",
    "        find_nearest_images_for_text(text, k, next_offset, in_widget)\n",
    "    def go_back(b):\n",
    "        find_nearest_images_for_text(text, k, prev_offset, in_widget)\n",
    "    try:\n",
    "        #print(\"displaying\", ids)\n",
    "        display_closest(ids[-k:], distances[-k:], load_more_cb=load_more, go_back_cb=go_back, in_widget=in_widget, offset=offset)\n",
    "    except Exception as e:\n",
    "        traceback.print_exc()\n",
    "    return True\n",
    "\n",
    "\n",
    "#open_in_finder(root_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059c163b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "images_hbox = widgets.HBox([])\n",
    "display(images_hbox)\n",
    "\n",
    "\n",
    "interact(find_nearest_images_for_text, text='', k=widgets.IntSlider(value=10, \n",
    "                                                                    min=1, \n",
    "                                                                    max=100,\n",
    "                                                                    layout={'width':'600px'}\n",
    "                                                                    \n",
    "                                                                   ), \n",
    "         in_widget=fixed(images_hbox), \n",
    "            offset=fixed(0))\n",
    "                  \n",
    "\n",
    "\n",
    "#find_nearest_images_for_text(\"test\", k=10, offset=20, in_widget=images_hbox)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4aa58ad",
   "metadata": {},
   "source": [
    "## benchmark mps performance (spoiler: it's 2x faster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a2a7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# benchmark.py\n",
    "import time\n",
    "class benchmark(object):\n",
    "    \"\"\" \n",
    "    usage:\n",
    "        with benchmark(log_string):\n",
    "            code_to_benchmark\n",
    "    \"\"\"\n",
    "    def __init__(self,name):\n",
    "        self.name = name\n",
    "    def __enter__(self):\n",
    "        self.start = time.time()\n",
    "    def __exit__(self,ty,val,tb):\n",
    "        end = time.time()\n",
    "        print(\"%s : %0.3f seconds\" % (self.name, end-self.start))\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6a803e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_benchmark():\n",
    "    print(\"warming up...\")\n",
    "    write_image_feature_vec([\"test-image-2.jpeg\"])\n",
    "    print(\"running benchmark...\")\n",
    "    with benchmark(device + 'batched'):\n",
    "        write_image_feature_vec([\"test-image.webp\", \"test-image-2.jpeg\"])\n",
    "        write_image_feature_vec([\"test-image.webp\", \"test-image-2.jpeg\"])\n",
    "        write_image_feature_vec([\"test-image.webp\", \"test-image-2.jpeg\"])\n",
    "    with benchmark(device + 'unbatched'):\n",
    "        write_image_feature_vec([\"test-image.webp\"])\n",
    "        write_image_feature_vec([\"test-image-2.jpeg\"])\n",
    "        write_image_feature_vec([\"test-image.webp\"])\n",
    "        write_image_feature_vec([\"test-image-2.jpeg\"])\n",
    "        write_image_feature_vec([\"test-image.webp\"])\n",
    "        write_image_feature_vec([\"test-image-2.jpeg\"])\n",
    "\n",
    "device='mps'\n",
    "print(\"moving model to\",device)\n",
    "model.to(device)\n",
    "run_benchmark()\n",
    "\n",
    "\n",
    "device='cpu'\n",
    "print(\"moving model to\",device)\n",
    "model.to(device)\n",
    "run_benchmark()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
