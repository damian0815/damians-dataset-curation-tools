{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a053ddb4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install nltk gensim numpy ipywidgets\n",
    "import numpy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4331fbd8",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5362a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "def load_prompts(path):\n",
    "    image_extensions = [\".jpg\", \".png\", \".jpeg\"]\n",
    "    for root, _, files in os.walk(path, topdown=False):\n",
    "        for file in files:\n",
    "            filename_without_extension, extension = os.path.splitext(file)\n",
    "            if extension not in image_extensions:\n",
    "                continue\n",
    "            image_path = os.path.join(root, file)\n",
    "            caption_path = os.path.join(root, filename_without_extension + \".txt\")\n",
    "            try:\n",
    "                with open(caption_path, 'r') as caption_file:\n",
    "                    caption = caption_file.read().replace('\\n', ' ')\n",
    "                    yield image_path, caption_path, caption\n",
    "            except FileNotFoundError:\n",
    "                continue\n",
    "                \n",
    "def reload_data(dataset_path):\n",
    "    prompts = []\n",
    "    image_filenames = []\n",
    "    for image_path, caption_path, caption in load_prompts(dataset_path):\n",
    "        prompts.append(caption)\n",
    "        image_filenames.append(image_path)\n",
    "    return prompts, image_filenames\n",
    "    \n",
    "\n",
    "prompts = []\n",
    "image_filenames = []\n",
    "\n",
    "#with open('data.txt', 'r') as file:\n",
    "#    data = file.read().replace('\\n', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2dc240b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path=\"./images\"\n",
    "prompts, image_filenames = reload_data(dataset_path)\n",
    "\n",
    "prompts_split_to_words = [p.replace(',', ' , ').split() for p in prompts]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02dd8619",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Union\n",
    "\n",
    "\n",
    "def sequence_is_in(pattern: Union[tuple,list], sequence: list):\n",
    "    if type(pattern) is tuple and type(sequence) is list:\n",
    "        pattern = list(pattern)\n",
    "    if type(sequence) is not list:\n",
    "        raise ValueError(f\"sequence {sequence} must be a list\")\n",
    "    if type(pattern) is not list:\n",
    "        raise ValueError(f\"unhandled type {type(pattern).__name__} for pattern\")\n",
    "    for i in range(len(sequence) - len(pattern) + 1):\n",
    "        if sequence[i:i+len(pattern)] == pattern:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "@dataclass\n",
    "class TermGroup: \n",
    "    terms: list[tuple[str]]\n",
    "    synonyms: list[list[tuple[str]]]\n",
    "        \n",
    "    def get_synonyms(self, term: tuple[str]) -> list[tuple[str]]:\n",
    "        \"\"\"\n",
    "        for the given term, return it and all of its synonyms (if any) as a list\n",
    "        \"\"\"\n",
    "        try:\n",
    "            synonym_idx = next(i for i,l in enumerate(self.synonyms) if sequence_is_in(term, l))\n",
    "            return self.synonyms[synonym_idx]\n",
    "        except StopIteration:\n",
    "            return [term]\n",
    "        \n",
    "    \n",
    "    def prompt_contains_terms(self, prompt: list[str]) -> list[bool]:\n",
    "        \"\"\"\n",
    "        for the given prompt, return an array of bools  of each of our terms\n",
    "        \"\"\"\n",
    "        result = [False] * len(self.terms)\n",
    "        for term_index,term in enumerate(self.terms):\n",
    "            for search_sequence in self.get_synonyms(term):\n",
    "                #print(f\"looking for {search_sequence} in {prompt}\")\n",
    "                if sequence_is_in(search_sequence, prompt):\n",
    "                    #print('found')\n",
    "                    result[term_index] = True\n",
    "                    break\n",
    "        return result\n",
    "    \n",
    "    def get_term_appearances(self, prompts: list[list[str]]) -> list[int]:\n",
    "        per_term_ids = [[] for _ in range(len(self.terms))]\n",
    "        for prompt_index,prompt in enumerate(prompts):\n",
    "            contains_term = self.prompt_contains_terms(prompt)\n",
    "            #print(contains_term)\n",
    "            matching_term_indices = [i for i,term in enumerate(self.terms) if contains_term[i]]\n",
    "            #print(f\"existing: {result_tuple}, {len(result_tuple[1])} prompts\")\n",
    "            for term_index in matching_term_indices:\n",
    "                per_term_ids[term_index].append(prompt_index)\n",
    "        return per_term_ids\n",
    "\n",
    "\n",
    "    \n",
    "def count_groups_appearances(prompts: list[list[str]], groups: list[TermGroup]) -> list[tuple[TermGroup, list[int]]]:\n",
    "    #result = [(group,[[] for _ in range(len(group.terms))]) for group in groups]\n",
    "    return [(group, group.get_term_appearances(prompts)) for group in groups]\n",
    "    #for prompt_index,prompt in enumerate(prompts):\n",
    "    #    for group_index,group in enumerate(groups):\n",
    "            \n",
    "    #        count_group_appearances(group, prompts)\n",
    "        #break\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b2d24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "groups = [\n",
    "    TermGroup(terms =[('flying', 'type'), ('psychic', 'type'), ('ground', 'type')], synonyms=[]),\n",
    "    TermGroup(terms =[('red',), ('yellow',), ('blue',), ('green',)], synonyms=[]),\n",
    "]\n",
    "\n",
    "appearances = count_groups_appearances(prompts_split_to_words, groups)\n",
    "print([(group,[len(ids) for ids in prompt_ids]) for group,prompt_ids in appearances])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2310365e",
   "metadata": {},
   "source": [
    "## optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10515ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def percentage_to_error(percentage: float, base=0.333) -> float:\n",
    "    powed = math.pow(base, percentage)\n",
    "    # remap 1..base to 1..0\n",
    "    return max(0, min(1, (powed-base) / (1.0-base)))\n",
    "    \n",
    "\n",
    "def get_group_score(group: TermGroup, permutation_prompts: list[list[str]], verbose = False) -> float:\n",
    "    \"\"\"\n",
    "    return a score based on how evenly shared the group is (lower is better)\n",
    "    \"\"\"\n",
    "    badness = 0\n",
    "    \n",
    "    appearances = group.get_term_appearances(permutation_prompts)\n",
    "    total_count = sum([len(l) for l in appearances])\n",
    "\n",
    "    target_percentage_per_term = 1/len(group.terms)\n",
    "    for term_index in range(len(appearances)):\n",
    "        term_ids = appearances[term_index]\n",
    "        percentage = len(term_ids) / (total_count+1)\n",
    "        error = percentage_to_error(percentage / target_percentage_per_term, base=10)\n",
    "        if verbose:\n",
    "            print(f\" - pct coverage badness: {error}, count: {round(100 * percentage)}%: {group.terms[term_index]} ({len(term_ids)} prompts)\")\n",
    "        badness += abs(error)\n",
    "\n",
    "    return badness/len(group.terms)\n",
    "    \n",
    "def get_example_count_score(group: TermGroup, \n",
    "                            permutation_prompts: list[list[str]], \n",
    "                            target_example_count: int=30, \n",
    "                            verbose = False) -> float:\n",
    "    \"\"\"\n",
    "    return a score that is the sum of whether the terms in this group have sufficient examples (lower is better)\n",
    "    \"\"\"\n",
    "    badness = 0\n",
    "    appearances = group.get_term_appearances(permutation_prompts)\n",
    "    for term_index in range(len(group.terms)):\n",
    "        example_count = len(appearances[term_index])\n",
    "        this_badness = percentage_to_error(example_count/target_example_count, base=5)\n",
    "        if verbose:\n",
    "            print(f\" - count badness: {this_badness}, count: {example_count}\")\n",
    "        badness += this_badness\n",
    "\n",
    "    return badness\n",
    "\n",
    "    \n",
    "def get_score(groups: list[TermGroup], \n",
    "              permutation_prompts: list[list[str]], \n",
    "              all_prompts_count: int, \n",
    "              verbose: bool = False) -> float:\n",
    "    \n",
    "    badness = 0\n",
    "    \n",
    "    # each group gets a score based on how evenly spread it is\n",
    "    coverage_badness = sum([get_group_score(group, permutation_prompts, verbose=verbose) for group in groups])\n",
    "    badness += coverage_badness * 10\n",
    "    \n",
    "    # each term gets a score based on how many examples it includes\n",
    "    example_count_badness = sum([get_example_count_score(group, permutation_prompts, verbose=verbose) for group in groups])\n",
    "    badness += example_count_badness * 1\n",
    "    \n",
    "    # the total cut gets a scored base on what percentage of the total prompt count it includes\n",
    "    taken_badness = percentage_to_error(len(permutation_prompts)/all_prompts_count)\n",
    "    if verbose:\n",
    "        print(f\"permutation uses {len(permutation_prompts)} prompts out of {all_prompts_count} -> badness {taken_badness}\")\n",
    "    badness += taken_badness*1\n",
    "    \n",
    "    return badness\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf58d7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = get_score(groups, \n",
    "                  permutation_prompts=prompts_split_to_words[:500], \n",
    "                  all_prompts_count=len(prompts_split_to_words),\n",
    "                 verbose = True)\n",
    "print(\"score: \", score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fa5d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "random.seed(101)\n",
    "\n",
    "def find_best_monte_carlo(groups: list[TermGroup], \n",
    "                          prompts_split_to_words: list[list[str]], \n",
    "                          num_iterations: int=100,\n",
    "                          ):\n",
    "    \n",
    "    num_ids = len(prompts_split_to_words)\n",
    "    def get_available_ids(selected_ids: list[int]) -> set[int]:\n",
    "        return set([id for id in range(num_ids) if id not in current_selected_ids])\n",
    "\n",
    "    def get_prompts_split_to_words_for_permutation(selected_ids: list[int]) -> list[list[str]]:\n",
    "        return [prompts_split_to_words[i] for i in selected_ids]\n",
    "    \n",
    "    def get_score_for_permutation_ids(permutation: list[int]) -> float:\n",
    "        return get_score(groups, get_prompts_split_to_words_for_permutation(current_selected_ids), num_ids)\n",
    "\n",
    "    current_selected_ids = set(random.sample(range(num_ids), num_ids//2))\n",
    "    current_score = get_score_for_permutation_ids(current_selected_ids)\n",
    "    \n",
    "    annealing_base = 0.5\n",
    "    def get_current_annealing_factor(iteration):\n",
    "        x = iteration / num_iterations\n",
    "        return (math.pow(annealing_base, x)-annealing_base) / (1-annealing_base)\n",
    "\n",
    "    \n",
    "    for iteration in tqdm(range(num_iterations)):\n",
    "        # remove and add a random number of elements\n",
    "        current_available_ids = get_available_ids(current_selected_ids)\n",
    "        permutation = current_selected_ids.copy()\n",
    "        \n",
    "        max_add_remove_count = max(1,int(0.25 * num_ids * get_current_annealing_factor(iteration)))\n",
    "        \n",
    "        if len(permutation) > 0:\n",
    "            to_remove_count = random.randrange(min(max_add_remove_count, len(current_selected_ids)))\n",
    "            for to_remove in random.sample(permutation, to_remove_count):\n",
    "                permutation.remove(to_remove)\n",
    "        if len(current_available_ids) > 0:\n",
    "            to_add_count = random.randrange(min(max_add_remove_count, len(current_available_ids)))\n",
    "            for to_add in random.sample(current_available_ids, to_add_count):\n",
    "                permutation.add(to_add)\n",
    "        #print(f\"trying {permutation}\")\n",
    "        permutation_score = get_score(groups, get_prompts_split_to_words_for_permutation(permutation), num_ids)\n",
    "        if permutation_score < current_score:\n",
    "            #print(f\"score {score} is better than current best {current_score}\")\n",
    "            current_score = permutation_score\n",
    "            current_selected_ids = permutation\n",
    "        if iteration % (num_iterations/10) == 0:\n",
    "            print(f\"current score: {current_score}\")\n",
    "        #print(f\"score {score} is better than current best {current_score}\")\n",
    "\n",
    "            \n",
    "    return current_selected_ids\n",
    "\n",
    "best_ids = find_best_monte_carlo(groups, prompts_split_to_words, num_iterations=1000)\n",
    "#print(f\"best: {best_ids}\")\n",
    "\n",
    "best_prompts_split_to_words = [prompts_split_to_words[i] for i in best_ids]\n",
    "score = get_score(groups, best_prompts_split_to_words, len(prompts_split_to_words), True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96204bf1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727b32f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
