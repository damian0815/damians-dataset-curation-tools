{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a053ddb4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install nltk gensim numpy ipywidgets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4331fbd8",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5362a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "dataset_path=\"./images\"\n",
    "\n",
    "import os\n",
    "def load_prompts(path):\n",
    "    image_extensions = [\".jpg\", \".png\", \".jpeg\"]\n",
    "    for root, _, files in os.walk(path, topdown=False):\n",
    "        for file in files:\n",
    "            filename_without_extension, extension = os.path.splitext(file)\n",
    "            if extension not in image_extensions:\n",
    "                continue\n",
    "            image_path = os.path.join(root, file)\n",
    "            caption_path = os.path.join(root, filename_without_extension + \".txt\")\n",
    "            try:\n",
    "                with open(caption_path, 'r') as caption_file:\n",
    "                    caption = caption_file.read().replace('\\n', ' ')\n",
    "                    yield image_path, caption_path, caption\n",
    "            except FileNotFoundError:\n",
    "                continue\n",
    "\n",
    "prompts = []\n",
    "image_filenames = []\n",
    "\n",
    "#with open('data.txt', 'r') as file:\n",
    "#    data = file.read().replace('\\n', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce381a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<style>\n",
    "textarea, input {\n",
    "    font-family: Arial, Helvetica, sans-serif;\n",
    "}\n",
    ":root {\n",
    "    --jp-ui-font-size1: '2em';\n",
    "}\n",
    "</style>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5532064",
   "metadata": {},
   "source": [
    "# build vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87858843",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import words\n",
    "from nltk import Text\n",
    "import itertools\n",
    "#import nltk\n",
    "#nltk.download('words')\n",
    "\n",
    "words_nested = []\n",
    "words = []\n",
    "word_ids = []\n",
    "nltk_text = None\n",
    "\n",
    "def rebuild_words_and_words_nested():\n",
    "    global words_nested\n",
    "    global words\n",
    "    words_nested = [p.replace(',', ' , ').split() for p in prompts]\n",
    "    words = list(itertools.chain.from_iterable(words_nested))\n",
    "    \n",
    "\n",
    "def words_to_word_ids(sentence: list):\n",
    "    return [nltk_text.index(w) for w in sentence]\n",
    "\n",
    "def word_ids_to_words(word_ids: list[int]):\n",
    "    return [nltk_text.tokens[wid] for wid in word_ids]\n",
    "\n",
    "def reload_data(dataset_path):\n",
    "    global prompts\n",
    "    global image_filenames\n",
    "    global nltk_text\n",
    "    global word_ids\n",
    "    prompts = []\n",
    "    image_filenames = []\n",
    "    for image_path, caption_path, caption in load_prompts(dataset_path):\n",
    "        prompts.append(caption)\n",
    "        image_filenames.append(image_path)\n",
    "    rebuild_words_and_words_nested()\n",
    "    nltk_text = Text(words)\n",
    "    word_ids = [words_to_word_ids(s) for s in words_nested]\n",
    "    \n",
    "reload_data(dataset_path)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029ad5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def register_caption_callback(image_id, callback):\n",
    "    caption_callbacks.get(image_id, []).append(callback)\n",
    "\n",
    "def set_caption(image_id, caption):\n",
    "    prompts[image_id] = caption\n",
    "    rebuild_words_and_words_nested()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a214391",
   "metadata": {},
   "source": [
    "# build ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daec1f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ngrams, FreqDist\n",
    "\n",
    "def get_sentence_ngrams(sentence: list, n: int=3) -> list[list[int]]:\n",
    "    grams = [x for x in ngrams(sentence, n)]\n",
    "    return grams\n",
    "\n",
    "def get_ngram_freq_dist_sorted_by_count(ngrams):\n",
    "    fdist = FreqDist()\n",
    "    for ngram in ngrams:\n",
    "        fdist[ngram] += 1\n",
    "    return fdist\n",
    "\n",
    "def get_all_ngrams_per_sentence(ngram_length: int) -> list[tuple]:\n",
    "    all_ngrams_nested_words = [get_sentence_ngrams(s, ngram_length) for s in words_nested]\n",
    "    return all_ngrams_nested_words\n",
    "\n",
    "def get_all_ngrams_flat(ngram_length: int) -> list[tuple[int, list[str]]]:\n",
    "    all_ngrams_nested_words = get_all_ngrams_per_sentence(ngram_length)\n",
    "    all_ngrams_flat_words = list(itertools.chain.from_iterable(all_ngrams_nested_words))\n",
    "    return all_ngrams_flat_words\n",
    "    \n",
    "def count_ngrams_containing_word(ngram_length: int, search_word: str) -> list[tuple[int, list[str]]]:\n",
    "    all_ngrams_flat_words = get_all_ngrams_flat(ngram_length)\n",
    "    fdist = get_ngram_freq_dist_sorted_by_count([ngram for ngram in all_ngrams_flat_words if search_word in ngram])\n",
    "    for ngram, count in fdist.most_common(50):\n",
    "        yield (count, ngram)\n",
    "        \n",
    "def find_prompts_containing_ngram(ngram_length: int, ngram_words: list[str]) -> list[int]:\n",
    "    all_ngrams_nested_words = [get_sentence_ngrams(s, ngram_length) for s in words_nested]\n",
    "    return [i for (i, ngrams) in enumerate(all_ngrams_nested_words) if ngram_words in ngrams]\n",
    "\n",
    "\n",
    "def count_ngrams(ngram_length: int):\n",
    "    all_ngrams_flat_words = get_all_ngrams_flat(ngram_length)\n",
    "    fdist = get_ngram_freq_dist_sorted_by_count(all_ngrams_flat_words)\n",
    "    return fdist\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85116d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "reload_data(dataset_path)\n",
    "all_ngrams_fdist = count_ngrams(3)\n",
    "\n",
    "least_to_most = [(k,v) for k,v in sorted(all_ngrams_fdist.items(), key=lambda x: x[1])]\n",
    "\n",
    "print(\"most common:\")\n",
    "print(\"\\n\".join(reversed([str(x) for x in least_to_most[-10:]])))\n",
    "print(\"least common:\")\n",
    "print(\"\\n\".join([str(x) for x in least_to_most[:10]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba876bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sentences_simple(search_word: str) -> [int]:\n",
    "    return [i for i, words in words_nested if search_word in words_nested]\n",
    "\n",
    "def find_sentences_complex(with_words: list[str], without_words: list[str]=[]) -> set[int]:\n",
    "    positive_matches = set([find_sentences_simple(w) for word in with_words])\n",
    "    negative_matches = set([find_sentences_simple(w) for word in without_words])\n",
    "    return positive_matches.minus(negative_matches)\n",
    "                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7caa56f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db90bedb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "\n",
    "import asyncio\n",
    "\n",
    "class Timer:\n",
    "    def __init__(self, timeout, callback):\n",
    "        self._timeout = timeout\n",
    "        self._callback = callback\n",
    "\n",
    "    async def _job(self):\n",
    "        await asyncio.sleep(self._timeout)\n",
    "        self._callback()\n",
    "\n",
    "    def start(self):\n",
    "        self._task = asyncio.ensure_future(self._job())\n",
    "\n",
    "    def cancel(self):\n",
    "        self._task.cancel()\n",
    "\n",
    "def debounce(wait):\n",
    "    \"\"\" Decorator that will postpone a function's\n",
    "        execution until after `wait` seconds\n",
    "        have elapsed since the last time it was invoked. \"\"\"\n",
    "    def decorator(fn):\n",
    "        timer = None\n",
    "        def debounced(*args, **kwargs):\n",
    "            nonlocal timer\n",
    "            def call_it():\n",
    "                fn(*args, **kwargs)\n",
    "            if timer is not None:\n",
    "                timer.cancel()\n",
    "            timer = Timer(wait, call_it)\n",
    "            timer.start()\n",
    "        return debounced\n",
    "    return decorator\n",
    "\n",
    "def make_caption_edit_box(image_id: int):\n",
    "    txtsl = widgets.Textarea(\n",
    "        value=prompts[image_id],\n",
    "        layout={'width': '800px', 'height': '100px'}\n",
    "    )\n",
    "    txtsl.style.font_size = '1.5em'\n",
    "    txtsl.image_id = image_id\n",
    "    txtsl.observe(textarea_contents_did_change, names='value')\n",
    "    \n",
    "    label_text = image_filenames[image_id] + \":\"\n",
    "    label = widgets.Label(label_text)\n",
    "    label.style.font_family = 'courier'\n",
    "    \n",
    "    vbox = widgets.VBox([label, txtsl])\n",
    "    return vbox\n",
    "\n",
    "\n",
    "\n",
    "@debounce(0.5)\n",
    "def textarea_contents_did_change(observation):\n",
    "    #print('changed:', observation)\n",
    "    new_caption = observation['new']\n",
    "    image_id = observation['owner'].image_id\n",
    "    set_caption(image_id, new_caption)\n",
    "    print('stored caption for', image_id)\n",
    "\n",
    "    \n",
    "def count_ngrams_and_display_editboxes(ngram_length:int, word:str, max_count:int):\n",
    "    vboxes = []\n",
    "    titles = []\n",
    "    for x in count_ngrams_containing_word(ngram_length, word):\n",
    "        count, ngram = x\n",
    "        if ngram[0] == ',':\n",
    "            continue\n",
    "        titles.append(f\"{count} captions contain {ngram}\")\n",
    "        if count > max_count:\n",
    "            vbox = widgets.VBox([])\n",
    "            vboxes.append(vbox)\n",
    "        else:\n",
    "            image_ids = find_prompts_containing_ngram(2, ngram)\n",
    "            edit_boxes = [make_caption_edit_box(image_id) for image_id in image_ids]\n",
    "            vbox = widgets.VBox(edit_boxes)\n",
    "            vboxes.append(vbox)\n",
    "    \n",
    "    accordian = widgets.Accordion(vboxes, titles=titles)\n",
    "    display(accordian)\n",
    "\n",
    "\n",
    "interact(count_ngrams_and_display_editboxes, \n",
    "         ngram_length=widgets.IntSlider(value=2, min=2, max=10, description='Length'), \n",
    "         word=widgets.Text(value='', description='Word', placeholder='type search word here'), \n",
    "         max_count=widgets.IntText(value=5, description='Max count')\n",
    "        )\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20eb529",
   "metadata": {},
   "source": [
    "# cluster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c43c6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import cluster\n",
    "from nltk.cluster import euclidean_distance\n",
    "from numpy import array\n",
    "import numpy\n",
    "\n",
    "# initialise the clusterer (will also assign the vectors to clusters) \n",
    "vectors = [array(f) for f in all_ngrams_flat]\n",
    "\n",
    "num_clusters = 10\n",
    "clusterer = cluster.KMeansClusterer(num_clusters, euclidean_distance, avoid_empty_clusters=True) \n",
    "clusters = clusterer.cluster(vectors, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26510a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "unique, counts = numpy.unique(clusters, return_counts=True)\n",
    "\n",
    "print(dict(zip(unique, counts)))\n",
    "\n",
    "\n",
    "\n",
    "#for cluster_index in range(0, num_clusters):\n",
    "#    count = numpy.\n",
    "\n",
    "#print(vectors[0],vectors[1])\n",
    "#diff = vectors[1]-vectors[0]\n",
    "#numpy.dot(diff, diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6830b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_cluster_contents(which_cluster):\n",
    "    contents = {}\n",
    "    counter = 0\n",
    "    for ngram_index, cluster in enumerate(clusters):\n",
    "        if cluster != which_cluster:\n",
    "            continue\n",
    "        ngram = all_ngrams_flat[ngram_index]\n",
    "        #print(ngram)\n",
    "        #print(nltk_text.words(list(ngram)))\n",
    "        # d[i] = d.setdefault(i, 0) + 1\n",
    "        #print(contents)\n",
    "        if ngram not in contents.keys():\n",
    "            contents.update({ngram: 1})\n",
    "        else:\n",
    "            contents.update({ngram: contents.get(ngram) + 1})\n",
    "        #print(contents)\n",
    "        #counter += 1\n",
    "        #if counter > 30:\n",
    "        #    break\n",
    "    return contents\n",
    "\n",
    "\n",
    "#for cluster_index in range(num_clusters):\n",
    "#    this_cluster_contents = []\n",
    "#    for ngram_index, ngram in enumerate(all_ngrams_flat):\n",
    "#        print(f\"ngram {ngram} is in cluster {clusters[ngram_index]}\")\n",
    "#        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae7b088",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# sort by occurence count\n",
    "def get_clustered_ngrams_sorted_by_count(cluster_index: int):\n",
    "    cluster_contents = get_cluster_contents(cluster_index)\n",
    "    #print(cluster_contents)\n",
    "    x = cluster_contents.keys()\n",
    "    #print(cluster_index, x)\n",
    "    ngrams_sorted_by_occurence = sorted(cluster_contents.items(), key=lambda item: item[1], reverse=True)\n",
    "    return [(word_ids_to_words(list(k)), v) for k, v in ngrams_sorted_by_occurence]\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
