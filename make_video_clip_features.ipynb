{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582b045b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install opencv_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7538e270",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "#repo_id = \"laion/CLIP-ViT-H-14-laion2B-s32B-b79K\"\n",
    "#repo_id = \"laion/CLIP-ViT-g-14-laion2B-s12B-b42K\" # more modern CLIP model\n",
    "repo_id = \"openai/clip-vit-large-patch14-336\" # the CLIP model used for SD up to v1.5\n",
    "device = 'mps'\n",
    "\n",
    "print(\"loading model...\")\n",
    "model = CLIPModel.from_pretrained(repo_id)\n",
    "print(\"loading preprocessor...\")\n",
    "processor = CLIPProcessor.from_pretrained(repo_id)\n",
    "\n",
    "print(f\"sending to {device}...\")\n",
    "model.half().to(device)\n",
    "print(\"done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6971cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "def resize_image(fullsize_image: PIL.Image, min_edge_length: int, max_edge_length: int=None) -> PIL.Image:\n",
    "    transform = transforms.Resize(size=min_edge_length, max_size=max_edge_length)\n",
    "    return transform(fullsize_image)\n",
    "\n",
    "\n",
    "\n",
    "def get_clip_image_features(image: PIL.Image) -> torch.Tensor:\n",
    "    preprocess_results = processor(text=None, \n",
    "                                   images=image, \n",
    "                                   return_tensors=\"pt\", \n",
    "                                   padding=True, \n",
    "                                   device=device\n",
    "                                  )\n",
    "    pixel_values = preprocess_results.pixel_values\n",
    "    #print(pixel_values.device)\n",
    "    image_features = model.get_image_features(pixel_values = pixel_values.half().to(model.device))\n",
    "    return image_features\n",
    "\n",
    "def get_clip_text_features(text: str) -> torch.Tensor:\n",
    "    input = processor(text=[text], images=None, return_tensors=\"pt\", padding=True)\n",
    "    #print(input)\n",
    "    input_ids = input.input_ids.to(device)\n",
    "    return model.get_text_features(input_ids=input_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c840258",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from async_video_processor import AsyncVideoProcessor\n",
    "\n",
    "\n",
    "accumulated_results = {}\n",
    "out_data = {}\n",
    "\n",
    "def process_func(frame_cv_bgr):\n",
    "    frame_cv_rgb = cv2.cvtColor(np.array(frame_cv_bgr), cv2.COLOR_BGR2RGB)    \n",
    "    pil_image = PIL.Image.fromarray(frame_cv_rgb)\n",
    "    features = get_clip_image_features(resize_image(pil_image, min_edge_length= 512))\n",
    "    \n",
    "    return features.detach().cpu()\n",
    "\n",
    "def results_func(frame_index, data):\n",
    "    accumulated_results[frame_index] = data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418316bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def test_tqdm_manual():\n",
    "    pbar = tqdm(range(1000))\n",
    "    for i in range(1000):\n",
    "        pbar.update(1)\n",
    "        time.sleep(0.01)\n",
    "\n",
    "test_tqdm_manual()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04001178",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "async def write_clip_features(root_path):\n",
    "    global accumulated_results\n",
    "    print(\"walking\", root_path)\n",
    "    for directory, _, filenames in os.walk(root_path):\n",
    "        video_extensions = [\".mp4\"]\n",
    "        print('directory:', directory)\n",
    "        video_filenames = [f for f in filenames if os.path.splitext(f)[1] in video_extensions]\n",
    "        if len(video_filenames)==0:\n",
    "            continue\n",
    "        for filename in tqdm(video_filenames, desc=directory):\n",
    "            video_path = os.path.join(directory, filename)\n",
    "            pickle_path = video_path + \".clip-features.pickle\"\n",
    "            if os.path.exists(pickle_path):\n",
    "                print(\"not overwriting existing\", pickle_path)\n",
    "                continue\n",
    "\n",
    "            accumulated_results = {}\n",
    "            process_fps = 0.5\n",
    "            first_frame_to_process = 0\n",
    "\n",
    "            def write_results_func(video, partial:bool):\n",
    "                if partial:\n",
    "                    return\n",
    "                out_data = {\n",
    "                    'type': 'clip features ' + repo_id,\n",
    "                    'fps': video.get(cv2.CAP_PROP_FPS),\n",
    "                    'features': accumulated_results,\n",
    "                    'frameIncrement': frame_increment\n",
    "                }\n",
    "                \n",
    "                with open(pickle_path, 'wb') as handle:\n",
    "                    pickle.dump(out_data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                print(\"cumulative detection count:\",str(len(accumulated_results)))\n",
    "\n",
    "            async_video_processor = AsyncVideoProcessor(video_path, process_func, results_func, write_results_func, first_frame_to_process, process_fps)\n",
    "            frame_increment = async_video_processor.frameIncrement\n",
    "            await async_video_processor.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c14a99b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "await write_clip_features(\"./videos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748e0fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from dataclasses import dataclass, field\n",
    "import nmslib\n",
    "\n",
    "@dataclass\n",
    "class ClipFeatureDataCollection:\n",
    "    paths: list[str] = field(default_factory=list)\n",
    "    feature_datas: list[ClipFeatureData] = field(default_factory=list)\n",
    "    id_offsets: list[int] = field(default_factory=list)\n",
    "    current_id_offset: int = 0\n",
    "    index = None\n",
    "        \n",
    "    def add_feature_data(self, path: str, fd: ClipFeatureData):\n",
    "        self.paths.append(path)\n",
    "        self.feature_datas.append(fd)\n",
    "        self.id_offsets.append(self.current_id_offset)\n",
    "        self.current_id_offset += len(fd.features)\n",
    "    \n",
    "    def rebuild_index(self):\n",
    "        self.index = None\n",
    "        \n",
    "        all_features = [torch.tensor(cfd.features[key]) for cfd in self.feature_datas for key in sorted(cfd.features.keys())]\n",
    "        all_features_tensor = torch.cat(all_features)\n",
    "        self.index = build_bruteforce_index(all_features_tensor)\n",
    "    \n",
    "    def _get_fd_index_for_id(self, id) -> int:\n",
    "        for index in range(1,len(self.id_offsets)):\n",
    "            if id < self.id_offsets[index]:\n",
    "                # gone too far -> step back\n",
    "                return index-1\n",
    "        # last one\n",
    "        return len(self.id_offsets)-1\n",
    "    \n",
    "    def get_path_and_frame_for_id(self, id) -> (str, int):\n",
    "        index = self._get_fd_index_for_id(id)\n",
    "        fd = self.feature_datas[index]\n",
    "        feature_index = id - self.id_offsets[index]\n",
    "        frame_number = fd.frame_increment * feature_index\n",
    "        path = self.paths[index]\n",
    "        return path,frame_number\n",
    "\n",
    "    \n",
    "    def get_closest_frames(self, features: torch.Tensor, k=4) -> [(str, int)]:\n",
    "        ids, distances = self.index.knnQuery(features, k=k)\n",
    "        return [self.get_path_and_frame_for_id(i) for i in ids]\n",
    "        \n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ClipFeatureData:\n",
    "    features: dict[int, np.array]\n",
    "    fps: float\n",
    "    frame_increment: int\n",
    "    \n",
    "    def get_nearest_features(self, frame:int) -> np.array:\n",
    "        matching_frame = (frame // self.frame_increment) * self.frame_increment\n",
    "        next_frame = matching_frame + self.frame_increment\n",
    "        prev_frame = matching_frame - self.frame_increment\n",
    "        \n",
    "        matching_distance = abs(frame - matching_frame)\n",
    "        next_distance = abs(frame - next_frame)\n",
    "        prev_distance = abs(frame - prev_frame)\n",
    "        \n",
    "        if next_distance < matching_distance and next_frame in self.features:\n",
    "            print(\"next:\", next_frame)\n",
    "            return self.features[next_frame]\n",
    "        if prev_frame in self.features and (\n",
    "            # may need to catch valid frames after the last frame we've captured\n",
    "                prev_distance < matching_distance or matching_frame not in self.features\n",
    "            ):\n",
    "            print(\"prev:\", prev_frame)\n",
    "            return self.features[prev_frame]\n",
    "        if matching_frame in self.features:\n",
    "            print(\"at:\", matching_frame)\n",
    "            return self.features[matching_frame]\n",
    "            \n",
    "        raise ValueError(f\"frame {frame} not available (max {max(self.features.keys())})\")\n",
    "        \n",
    "def load_clip_features(video_path):\n",
    "    pickle_path = video_path + \".clip-features.pickle\"\n",
    "    with open(pickle_path, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "        return ClipFeatureData(features = data['features'], \n",
    "                               fps = data['fps'], \n",
    "                               frame_increment = data['frameIncrement'])\n",
    "\n",
    "def build_fancy_index(all_features_tensor) -> nmslib:\n",
    "    return build_index(all_features_tensor, method='hnsw')\n",
    "    \n",
    "def build_bruteforce_index(all_features_tensor: torch.Tensor) -> nmslib:\n",
    "    return build_index(all_features_tensor, method='brute_force')\n",
    "\n",
    "def build_index(all_features_tensor: torch.Tensor, method:str = 'brute_force') -> nmslib:\n",
    "    print(\"building index for all_features_tensor of shape\", all_features_tensor.shape)\n",
    "    index = nmslib.init(method=method, space='cosinesimil')\n",
    "    index.addDataPointBatch(all_features_tensor.cpu())\n",
    "    opts = {}\n",
    "    if method == 'hnsw':\n",
    "        args['post'] = 2\n",
    "    index.createIndex(opts, print_progress=True)\n",
    "    return index\n",
    "\n",
    "def get_closest_images(index: nmslib, features: np.array, k=4):\n",
    "    ids, distances = index.knnQuery(features, k=k)\n",
    "    #print(\"knnquery got ids\", ids)\n",
    "    return ids, distances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7305d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "video_path = \"./videos/hotline-bling.mp4\"\n",
    "data = load_clip_features(video_path)\n",
    "all_features_tensor = torch.cat([torch.tensor(f) for f in data.features.values()])\n",
    "index = build_bruteforce_index(all_features_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ba6028",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import PIL\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "def load_video(video_path):\n",
    "    video = cv2.VideoCapture(video_path)\n",
    "    if not video.isOpened():\n",
    "        raise ValueError(\"cannot open \" + video_path)\n",
    "\n",
    "    video.set(cv2.CAP_PROP_POS_AVI_RATIO,1)\n",
    "    total_frames = video.get(cv2.CAP_PROP_POS_FRAMES)\n",
    "    video.set(cv2.CAP_PROP_POS_AVI_RATIO,0)\n",
    "    \n",
    "    return video,total_frames\n",
    "\n",
    "\n",
    "def get_frame_pil(video, frame_number):\n",
    "    video.set(cv2.CAP_PROP_POS_FRAMES,frame_number)\n",
    "    ret, frame_bgr = video.read()\n",
    "    if not ret:\n",
    "        raise RuntimeError(f\"unable to read frame {frame_number}\")\n",
    "\n",
    "    frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n",
    "    return PIL.Image.fromarray(frame_rgb)\n",
    "    \n",
    "def resize_image(image, max_width, max_height) -> PIL.Image:\n",
    "    width, height = image.size\n",
    "    aspect = width/height\n",
    "    factor = max_width/width\n",
    "    if height * factor > max_height:\n",
    "        factor = max_height/height\n",
    "    return image.resize([int(width*factor), int(height*factor)])\n",
    "    \n",
    "def display_video_frame(video, frame_number, max_width, max_height):\n",
    "    display(resize_image(get_frame_pil(video, frame_number), max_width, max_height))\n",
    "    \n",
    "def display_nearest_frames(data:ClipFeatureData, index: nmslib, video, search_features, k=4):\n",
    "    ids, distances = get_closest_images(index, search_features, k=k)\n",
    "    print(f\"found ids: {ids} -> frames: {[data.frame_increment*i for i in ids]}\")\n",
    "    for frame_number in [data.frame_increment*i for i in ids]:\n",
    "        display_video_frame(video, frame_number, 300, 300)\n",
    "\n",
    "def get_combined_weighted_features(feature_tensors: [torch.Tensor], weights: [float]) -> torch.Tensor:\n",
    "    combined = torch.cat([t*weights[i] for i,t in enumerate(feature_tensors)])\n",
    "    print(combined.shape)\n",
    "    result = torch.sum(combined, dim=0) / sum(weights)\n",
    "    return result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae25eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "video, total_frames = load_video(video_path)\n",
    "search_string = \"a man on a staircase\"\n",
    "\n",
    "for i in range(3):\n",
    "    source_frame = random.randint(0, total_frames)\n",
    "    print(\"source:\")\n",
    "    display(resize_image(get_frame_pil(video, source_frame), 300, 300))\n",
    "    source_frame_features = data.get_nearest_features(source_frame)\n",
    "    text_features = get_clip_text_features(search_string)\n",
    "    \n",
    "    search_features = get_combined_weighted_features([source_frame_features, text_features.detach().cpu()],\n",
    "                                                    [0.1,0.9])\n",
    "    \n",
    "    print(\"nearest:\")\n",
    "    display_nearest_frames(data, index, video, search_features, k=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38b1b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_paths = [ \"./videos/hotline-bling.mp4\", \"./videos/b.mp4\" ]\n",
    "fpc = ClipFeatureDataCollection()\n",
    "for video_path in video_paths:\n",
    "    data = load_clip_features(video_path)\n",
    "    fpc.add_feature_data(video_path, data)\n",
    "fpc.rebuild_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3728aeb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "search_string = \"a man on a staircase\"\n",
    "text_features = get_clip_text_features(search_string).detach().cpu()\n",
    "for video_path, frame_number in fpc.get_closest_frames(text_features, k=10):\n",
    "    print(\"video_path: \", video_path)\n",
    "    video, _ = load_video(video_path)\n",
    "    display_video_frame(video, frame_number, 300, 300)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8f668f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
